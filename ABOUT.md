# Command Recommendation System

## 1. Introduction

The Command Recommendation System is designed to help users find the most relevant command from a dataset based on a given query. This project leverages the power of TF-IDF (Term Frequency-Inverse Document Frequency) and various similarity metrics to provide accurate and useful recommendations. The system is particularly useful for environments where users frequently need to recall or discover commands from a large set of options, such as command-line interfaces, programming language libraries, or configuration files.

## 2. Data

The command dataset used in this project is stored in `data/commands_dataset.txt`. It contains a collection of over 600 commands and their descriptions scraped from [this GitHub repository](https://github.com/trinib/Linux-Bash-Commands/blob/main/README.md). This dataset is used as database for recommendation system.

### Test Datasets

- `test1_dataset.txt`: Contains a list of queries for each command from the training dataset, generated by GPT-4.
- `test2_dataset.txt`: A completely synthetic dataset with 100 queries generated by GPT-4.
- `test3_dataset.txt`: Another synthetic dataset with 50 queries generated by GPT-4.

## 3. Technologies Used

This project is implemented in Python and utilizes the following tools and libraries:

- **NLTK**: Used for natural language processing tasks such as tokenization, lemmatization, stemming, and stopwords removal.
- **SciPy**: Provides scientific computing functionalities, including special functions for calculating KL Divergence and other metrics.
- **Pandas & NumPy**: Used for data manipulation and numerical operations.

## 4. Project Architecture

### 4.1 Data Preparation

In the initial phase, the data is preprocessed and prepared for model training. The command descriptions are processed using the following steps:

- **Lowercase Conversion**: Normalizing text to lowercase.
- **Removing Punctuation**: Cleaning the text from punctuation marks.
- **Tokenization**: Splitting text into tokens.
- **Removing Stopwords**: Filtering out common stopwords using NLTK.
- **Lemmatization** (optional): Reducing words to their base or root form.
- **Stemming** (optional): Reducing words to their word stem.

The preprocessed data is then used to calculate TF-IDF vectors for each command.

### 4.2 Model Preparation and Training Process

The project includes a `TFIDFEngine` class that handles the training and recommendation processes. The steps involved are:

- **IDF Calculation**: Computes the Inverse Document Frequency (IDF) for each term in the dataset.
- **TF-IDF Matrix Calculation**: Creates a TF-IDF vector representation for each command based on term frequencies and IDF values. Logaritmic versions TF and IDF are used.

### 4.3 Recommendation Process

The system allows users to input a query and receive the top N command recommendations based on the chosen similarity metric. The `recommend_command` function handles the recommendation logic and provides the most relevant commands along with their descriptions and scores.

Supported Similarity Metrics:
- **Sum**: Simple sum of TF-IDF scores.
- **Cosine Similarity**: Measures the cosine of the angle between two vectors.
- **KL Divergence**: Measures how one probability distribution diverges from a second.
- **Pearson Correlation**: Measures the linear correlation between two sets of data.
- **Jensen-Shannon Divergence**: A smoothed and symmetric version of KL Divergence.

## 5. Performance Evaluation
In this section, we present the top 3 accuracy results for each test dataset using different similarity metrics. We compare case with lemmatization and stemming (in this order) or without it. 

### With lemmatization and stemming:
| Metric | Test 1 | Test 2 | Test 3 |
|--------|--------|--------|--------|
| sum | 72.03% | 60.00% | 63.92% |
| cosine | 73.48% | 62.00% | 65.98% |
| kld | 38.41% | 32.00% | 39.18% |
| pearson | 68.84% | 58.00% | 63.92% |
| jsd | 73.04% | 62.00% | 65.98% |


### Without lemmatization and stemming:
| Metric | Test 1 | Test 2 | Test 3 |
|--------|--------|--------|--------|
| sum | 68.26% | 48.00% | 56.70% |
| cosine | 68.26% | 50.00% | 59.79% |
| kld | 32.32% | 16.00% | 29.90% |
| pearson | 64.78% | 48.00% | 57.73% |
| jsd | 68.55% | 48.00% | 58.76% |

### 6. Results discussion

The results indicate that all metrics (except one) perform similarly, likely because the descriptions in all datasets are of similar length. However, KL Divergence performs poorly, which could be attributed to the high penalties for words not present in the query (due to very small Q values that inflate the logarithm). 

Jensen-Shannon Divergence appears to be a more reliable metric, as it consistently yields high accuracy across all test sets. This is likely due to its symmetric and smoothed nature, making it more robust in handling the variations in the data.

Additionally, applying lemmatization and stemming slightly improves the accuracy of the recommendations. 